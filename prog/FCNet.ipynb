{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux pleinement connectés \n",
    "\n",
    "Cet exercice fait suite au devoir 1.  L'objectif ici est d'implanter un réseau pleinement connecté avec une approche plus modulaire. Pour chaque couche, vous serez appelé à implanter une fonction `forward` et une fonction `backward`. La fonction `forward` reçoit en entrée un tenseur `x`, des poids `w` et possiblement d'autres parameters, et retourne le tenseur de sortie de la couche `out`.  La fonction retourne aussi une variable `cache`  contenant des données utilisés pour la rétropropagation (fonction `backward`).  La structure de la fonction `forward` est la suivante : \n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "N'oubliez pas que `x` est une `batch` et donc conbient plus d'un élément.\n",
    "\n",
    "En rétropropagation, la fonction `backward` de la couche reçoit en entrée un tenseur de dérivées `dout` ainsi que la liste `cache` calculée lors de la propagation avant (fonction `forward`).  Elle retourne deux tenseurs de gradients: un par rapport à ses entrée (`dx`) et un par rapport à ses poids (`dw`) (et parfois un par rapport aux bias `db`).  La structure de la fonction `backward` est la suivante : \n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "Une fois ce type de couches implanté, il sera possible de les combiner ensemble et ainsi construire des réseaux de neurones de différentes architectures.\n",
    "\n",
    "En plus des réseaux pleinement connectés, nous explorerons différents algorithme de descente de gradient et  introduirons *Dropout* et *Batch Norm*.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run the following from the ift725 directory and try again:\n",
      "python setup.py build_ext --inplace\n",
      "You may also need to restart your iPython kernel\n",
      "setup done!\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "#import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ift725.classifiers.fc_net import *\n",
    "from ift725.data_utils import get_CIFAR10_data\n",
    "from ift725.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from ift725.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "print('setup done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couche pleinement connectée : propagation avant\n",
    "Dans le fichier `ift725/layers.py`, vous devez coder la fonction `forward_fully_connected` et la tester avec le code de la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-shape  =  (1, 2, 2, 3)\n",
      "w-shape  =  (12, 1)\n",
      "b-shape  =  (1,)\n",
      "out =  [[36.7]]\n",
      "correct_out =  36.7\n",
      "Testing forward_fully_connected function:\n",
      "difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the forward_fully_connected function : batch 1 and 1 output neuron\n",
    "\n",
    "input_shape = (2, 2, 3) # an 2x2x3 input variable (a CIFAR10 image would have a 32x32x3 shape)\n",
    "\n",
    "input_size = np.prod(input_shape) # here 12\n",
    "weight_size = np.prod(input_shape) # here 12\n",
    "\n",
    "x = np.floor(np.linspace(-0.1, 0.5, num=input_size).reshape(1, *input_shape)*10)\n",
    "w = np.floor(np.linspace(-0.2, 0.3, num=weight_size).reshape(weight_size, 1)*10)\n",
    "b = np.linspace(-0.3, 0.1, num=1)\n",
    "\n",
    "print('x-shape  = ', x.shape)\n",
    "print('w-shape  = ', w.shape)\n",
    "print('b-shape  = ', b.shape)\n",
    "\n",
    "out, _ = forward_fully_connected(x, w, b)\n",
    "correct_out = 36.7\n",
    "\n",
    "print('out = ', out)\n",
    "print('correct_out = ', correct_out)\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing forward_fully_connected function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-shape  =  (2, 2, 2, 3)\n",
      "w-shape  =  (12, 1)\n",
      "b-shape  =  (1,)\n",
      "out =  [[14.7]\n",
      " [20.7]]\n",
      "correct_out =  [14.7 20.7]\n",
      "Testing forward_fully_connected function:\n",
      "difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the forward_fully_connected function : batch 2 and 1 output neuron\n",
    "\n",
    "num_inputs = 2          # batch of 2 input variables\n",
    "input_shape = (2, 2, 3) # an 2x2x3 input variable (a CIFAR10 image would have a 32x32x3 shape)\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape) # here 2x12 = 24\n",
    "weight_size = np.prod(input_shape) # here 12\n",
    "\n",
    "x = np.floor(np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)*10)\n",
    "w = np.floor(np.linspace(-0.2, 0.3, num=weight_size).reshape(weight_size, 1)*10)\n",
    "b = np.linspace(-0.3, 0.1, num=1)\n",
    "\n",
    "print('x-shape  = ', x.shape)\n",
    "print('w-shape  = ', w.shape)\n",
    "print('b-shape  = ', b.shape)\n",
    "\n",
    "out, _ = forward_fully_connected(x, w, b)\n",
    "correct_out = np.array([14.7, 20.7])\n",
    "\n",
    "print('out = ', out)\n",
    "print('correct_out = ', correct_out)\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing forward_fully_connected function:')\n",
    "print('difference: ', rel_error(out.T, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-shape  =  (2, 2, 2, 3)\n",
      "w-shape  =  (12, 2)\n",
      "b-shape  =  (2,)\n",
      "out =  [[12.7 14.1]\n",
      " [11.7 23.1]]\n",
      "correct_out =  [[12.7 14.1]\n",
      " [11.7 23.1]]\n",
      "Testing forward_fully_connected function:\n",
      "difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the forward_fully_connected function : batch 2 and 2 output neurons\n",
    "num_inputs = 2          # batch of 2 input variables\n",
    "input_shape = (2, 2, 3) # each variable as a 2x2x3 shape (a CIFAR10 RBG image would have a 32x32x3 shape)\n",
    "output_dim = 2          # the output has 2 neurons\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)  #here 2x2x2x3 = 24\n",
    "weight_size = output_dim * np.prod(input_shape) #here 2x12 = 24\n",
    "\n",
    "x = np.floor(np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)*10)\n",
    "w = np.floor(np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)*10)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "print('x-shape  = ', x.shape)\n",
    "print('w-shape  = ', w.shape)\n",
    "print('b-shape  = ', b.shape)\n",
    "\n",
    "out, _ = forward_fully_connected(x, w, b)\n",
    "correct_out = np.array([[ 12.7,  14.1],\n",
    "                        [ 11.7,  23.1]])\n",
    "\n",
    "print('out = ', out)\n",
    "print('correct_out = ', correct_out)\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing forward_fully_connected function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couche pleinement connectée : rétro-propagation\n",
    "If faut maintenant implanter la fonction `backward_fully_connected` et tester votre code avec un gradient numérique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward_fully_connected function:\n",
      "dx error:  8.649332397601269e-10\n",
      "dw error:  3.061778263521519e-10\n",
      "db error:  9.98115782221932e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the backward_fully_connected function\n",
    "# Here a case for a batch of 10 elements\n",
    "# Each elements has a 2x3 size\n",
    "# The layer has 5 output neurons\n",
    "\n",
    "x = np.random.randn(10, 2, 3)  # batch of 10 elements, each of size 2x3\n",
    "w = np.random.randn(6, 5)      # 2x3=6 weigts times 5 output neurones\n",
    "b = np.random.randn(5)         # one bias for each output neurone\n",
    "dout = np.random.randn(10, 5)  # the upcoming gradient at each output neuron and for each element of the batch\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: forward_fully_connected(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: forward_fully_connected(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: forward_fully_connected(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = forward_fully_connected(x, w, b)\n",
    "dx, dw, db = backward_fully_connected(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing backward_fully_connected function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))  #Gradient with respect to the input x : size 10x2x3\n",
    "print('dw error: ', rel_error(dw_num, dw))  #Gradient with respect to the weights w : size 5x6\n",
    "print('db error: ', rel_error(db_num, db))  #Gradient with respect to the bias : size 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couche ReLU : propagation avant\n",
    "Il faut implanter la fonction d'activation ReLU avec la fonction `relu_forward`.  Testez votre implantation avec la cellule que voici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = forward_relu(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print('Testing forward_relu function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couche ReLU : rétropropagation\n",
    "Maintenant il faut implanter la rétro-propagation pour une fonction d'activation ReLU via la fonction `relu_backward`.  Testez votre implantation avec le gradient numérique que voici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(2, 2)\n",
    "dout = np.random.randn(*x.shape) # Upstream gradient that retropropagates at that layer\n",
    "print(\"x = \", x)\n",
    "print(\"dout  = \", dout)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: forward_relu(xx)[0], x, dout)\n",
    "\n",
    "_, cache = forward_relu(x)\n",
    "dx = backward_relu(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print('Testing backward_relu function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couches \"Combo\"\n",
    "Souvent, on combine une couche pleinement connectée avec une fonction d'activation comme ReLU. Afin de simplifier ces situations, nous avons différentes fonctions à cet effet dans `ift725/layer_combo.py`.\n",
    "\n",
    "Pour l'instant, nous porterons notre attention sur les fonctions `forward_fully_connected_transform_relu` et `backward_fully_connected_transform_relu`.  Vous pouvez vérifier le code à l'aide de la vérification numérique que voici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from ift725.layer_combo import forward_fully_connected_transform_relu, backward_fully_connected_transform_relu\n",
    "\n",
    "x = np.random.randn(2, 3, 4)  # Batch of 2 elements of size 3x4\n",
    "w = np.random.randn(12, 10)   # 10 output neurons, each associated with 12=3x4 weights\n",
    "b = np.random.randn(10)       # 10 biases\n",
    "dout = np.random.randn(2, 10) # up stream gradient for each neuron (10) and each batch element (2)\n",
    "\n",
    "out, cache = forward_fully_connected_transform_relu(x, w, b)\n",
    "dx, dw, db = backward_fully_connected_transform_relu(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: forward_fully_connected_transform_relu(xx, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda ww: forward_fully_connected_transform_relu(x, ww, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda bb: forward_fully_connected_transform_relu(x, w, bb)[0], b, dout)\n",
    "\n",
    "# Error should be around 1e-10\n",
    "print('Testing forward_fully_connected_transform_relu:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions de perte : Softmax et SVM\n",
    "Au devoir 1, vous avez implanter ces deux fonctions de perte.  Vous devez donc récupérer votre code et l'adapter aux fonctions `softmax_loss` et `svm_loss` du fichier `ift725/layers.py`.\n",
    "\n",
    "Afin de vous assurer que tout fonctionne pour le mieux, exécutez le code que voici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be 1e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau à deux couches\n",
    "Au tp1, vous avez implanté un réseau à deux couches à l'intérieur d'une seule classe monolitique. Maintenant que les couches ont un design plus modulaire, vous devez implanter un réseau à deux couches de façon modulaire.\n",
    "\n",
    "Avec le fichier `ift725/classifiers/fc_net.py`, vous devez compléter l'implantation de la classe `TwoLayerNeuralNet`. Le design de cette classe est le prototype pour les autres réseaux utilisés dans ce devoir.  Par conséquent, soyez attentifs et assurez-vous de bien comprendre cet API. Vous pouvez exécuter la cellule que voici pour tester votre code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets try a forward pass for a minibatch of 3 elements of size 5, with an hidden layer of size 50 and 7 classes\n",
    "\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-2\n",
    "model = TwoLayerNeuralNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "print('score difference = ', scores_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets compute the loss with and without regularization\n",
    "y = np.asarray([0, 5, 1])\n",
    "\n",
    "model.reg = 0.0  # NO REG\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "print('loss difference = ', abs(loss - correct_loss))\n",
    "\n",
    "model.reg = 1.0  # WITH REG\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 63.9539735065\n",
    "print('loss = ', loss, '  correct_loss = ', correct_loss)\n",
    "print('loss difference = ', abs(loss - correct_loss))\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets compute the loss with different regularization terms\n",
    "\n",
    "for reg in [0.0, 0.3, 0.6, 0.9]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "Au tp1, l'entraînement des modèles était couplé aux modèles. Suivant un design plus modulaire, dans ce tp nous avons séparé le code d'entraînement et le code des modèles dans différentes classes.\n",
    "\n",
    "Familiarisez-vous avec le code `ift725/solver.py` et assurez-vous de bien en comprendre le fonctionnement. Après, utilisez un `Solver` pour entraîner le `TwoLayerNeuralNet` et atteindre environ `50%` de justesse en validation sur le base de donnée CIFAR10 stockée dans la variable `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = TwoLayerNeuralNet()\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Utilisez une instance de Solver pour entrainer un TwoLayerNeuralNet  #\n",
    "#  qui atteint au moins 50% de précision sur l'ensemble de validation.       #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                             FIN DE VOTRE CODE                              #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de neurones multi-couches\n",
    "Maintenant vous devez implanter un réseau ayant un nombre arbitraire de couches.\n",
    "\n",
    "Pour ce faire, prenez connaissance de la classe `FullyConnectedNeuralNet` du fichier `ift725/classifiers/fc_net.py`.\n",
    "\n",
    "Vous devez implanter **l'initialization, la propagation avant et la rétro-propagation**. Pour le moment, ne vous souciez pas de dropout ni de batch norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fonction de perte et vérification du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prochaine cellule effectue une vérification dilligente.  Exécutez la cellule afin de vous assurez que la loss avec et sans régularisation fonctionne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ift725.classifiers.fc_net import *\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,)) \n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNeuralNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "    \n",
    "  # Relative error should be below 1e-5\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "\n",
    "Pourquoi croyez-vous que les résultats de la cellule précédentes font foi d'une bonne fonction de perte?\n",
    "     \n",
    "**Votre réponse:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autre vérification diligente, assurez-vous que votre code peut \"overfitter\" sur un petit ensemble de 50 images. Pour ce faire, essayons un réseau à 3 couches cachées ayant chacune 100 neurones. Une recherche d'hyper-paramètres sera effectué pour trouver le bon taux d'apprentissage (learning rate) ainsi que le `weight_scale`.  Vous devriez être capable d'atteindre une justesse en entraînement de 100% avec 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "best_weight_scale = 0.\n",
    "best_learning_rate = 0.\n",
    "best_training_acc = 0.\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Utilisez une instance de Solver pour entrainer un réseau à 3 couches #\n",
    "#  et 100 neurones par couche à overfitter 50 images de CIFAR10.  Il est     #\n",
    "#  suggéré d'effectuer une recherche d'hyperparamètres pour trouver le bon   #\n",
    "#  `learning_rate` et le bon `weight_scale`.                                 #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                             FIN DE VOTRE CODE                              #\n",
    "##############################################################################\n",
    "\n",
    "print('The best learning_rate, weight_scale and training accuracy are',best_weight_scale, best_learning_rate, best_training_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "# import math\n",
    "marker_size = 100\n",
    "x_scatter, x_label = [np.log10(x[0]) for x in results], 'log weight scale'\n",
    "y_scatter, y_label = [np.log10(x[1]) for x in results], 'log learning rate'\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title('Train accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, faites la même chose pour un réseau à 5 couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Utilisez une instance de Solver pour entrainer un réseau à 5 couches #\n",
    "#  à 100 neurones à overfitter 50 images de CIFAR10.  Il est suggérer de     #\n",
    "#  d'effectuer une recherche d'hyperparamètres pour trouver le bon           #\n",
    "#  `learning_rate` et le bon `weight_scale`.                                 #\n",
    "##############################################################################\n",
    "\n",
    "def uniform(minv, maxv):\n",
    "    return np.random.rand() * (maxv - minv) + minv\n",
    "\n",
    "results = {}\n",
    "for _ in range(40):\n",
    "    weight_scale = 10 ** uniform(-2, -1)\n",
    "    learning_rate = 10 ** uniform(-4, -2)\n",
    "    print(weight_scale, learning_rate, '\\n')\n",
    "    \n",
    "    model = FullyConnectedNeuralNet([100, 100, 100, 100],\n",
    "                  weight_scale=weight_scale, dtype=np.float64)\n",
    "    solver = Solver(model, small_data,\n",
    "                    print_every=10, num_epochs=20, batch_size=25,\n",
    "                    update_rule='sgd',\n",
    "                    optim_config={\n",
    "                      'learning_rate': learning_rate,\n",
    "                    }, verbose=False)\n",
    "    solver.train()\n",
    "    train_acc = solver.train_acc_history[-1]\n",
    "    print('final loss:', solver.loss_history[-1], ' train_acc:', train_acc)\n",
    "    print('\\n')\n",
    "    plt.plot(solver.loss_history)\n",
    "    results[(weight_scale, learning_rate)] = train_acc\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                             FIN DE VOTRE CODE                              #\n",
    "##############################################################################\n",
    "best_weight_scale = 0.\n",
    "best_learning_rate = 0.\n",
    "best_training_acc = 0.\n",
    "print('The best learning_rate, weight_scale and training accuracy are',best_weight_scale, best_learning_rate, best_training_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "# import math\n",
    "marker_size = 100\n",
    "x_scatter, x_label = [np.log10(x[0]) for x in results], 'log weight scale'\n",
    "y_scatter, y_label = [np.log10(x[1]) for x in results], 'log learning rate'\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title('Train accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient ++\n",
    "Jusqu'à présent nous avons utilisé l'algorithmes de base de la descente de gradient (SGD-stochastic gradient descent). Ici nous testerons d'autres algorithmes plus sophistiqués."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "SGD+momentum est très largement utilisé.  Ouvrez  `ift725/optim.py` et prenez connaissance du code et implémentez la fonction `sgd_momentum` et exécutez la cellule que voici. Votre erreur devrait être inféreieure à 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ift725.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "    [-0.39994,    -0.34737526, -0.29481053, -0.24224579, -0.18968105],\n",
    "    [-0.13711632, -0.08455158, -0.03198684,  0.02057789,  0.07314263],\n",
    "    [ 0.12570737,  0.17827211,  0.23083684,  0.28340158,  0.33596632],\n",
    "    [ 0.38853105,  0.44109579,  0.49366053,  0.54622526,  0.59879   ]])\n",
    "expected_velocity = np.asarray([\n",
    "    [-0.06 ,       0.00684211,  0.07368421,  0.14052632,  0.20736842],\n",
    "    [ 0.27421053,  0.34105263,  0.40789474,  0.47473684,  0.54157895],\n",
    "    [ 0.60842105,  0.67526316,  0.74210526,  0.80894737,  0.87578947],\n",
    "    [ 0.94263158,  1.00947368,  1.07631579,  1.14315789,  1.21      ]])\n",
    "\n",
    "# Error should be below 1e-06\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, excécutez cette cellule.  Normalement, le réseau à 6 couches devrait s'entraîner plus rapidement avec SGD+momentum qu'avec SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNeuralNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, '-o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp et Adam\n",
    "RMSProp [1] et Adam [2] sont d'autres algorithmes de descente de gradient dont le code est dans le fichier `ift725/optim.py`.   Alors que le code de Adam vous est fournit, vous devez rédigé celui de RMSProp.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  3.8665153708273904e-08\n",
      "cache error:  3.2856307779330615e-09\n"
     ]
    }
   ],
   "source": [
    "# Test RMSProp implementation; you should see errors less than 1e-7\n",
    "import numpy as np\n",
    "from ift725.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "    [-0.39,       -0.33846964, -0.2868865,  -0.23525427, -0.18357633],\n",
    "    [-0.13185574, -0.0800953,  -0.02829757,  0.02353511,  0.07540058],\n",
    "    [ 0.12729687,  0.17922215,  0.23117507,  0.28315533,  0.33516143],\n",
    "    [ 0.38719188,  0.43924528,  0.49132033,  0.54341585,  0.59553073]])\n",
    "expected_cache = np.asarray([\n",
    "    [0.6,        0.61510526, 0.63021053, 0.64531579, 0.66042105],\n",
    " [0.67552632, 0.69063158, 0.70573684, 0.72084211, 0.73594737],\n",
    " [0.75105263, 0.76615789, 0.78189474, 0.79805263, 0.81421053],\n",
    " [0.83036842, 0.84652632, 0.86268421, 0.87884211, 0.895     ]])  \n",
    "\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "from ift725.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, config = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter le code que voici afin de comparer ces différent algorithmes.  En pricipe Adam devrait être le meilleur algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNeuralNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînez un bon model!\n",
    "Entraînez le meilleur réseau de neurones possible sur CIFAR-10 et mettez le meilleur modèle dans la variable `best_model`. Vous devriez avoir au moins une justesse 50% (voire même 55%) en validation et en test.\n",
    "\n",
    "Plus tard dans le devoir, on vous demandera d'entrainer et de tester une réseau convolutionnel sur CIFAR-10.  Vous verrez alors que cette architecture est supérieure aux réseaus de neurones pleinement connectés.\n",
    "\n",
    "NOTE: il serait judicieux de compléter le notebook `BatchNormalization.ipynb` et `Dropout.ipynb` avant de compléter cette dernière partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_delete = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_delete:\n",
    "    trys = []\n",
    "    best_try = -1\n",
    "    best_val_acc = 0\n",
    "    print('Deleted trys')\n",
    "else:\n",
    "    print('Did not delete trys')\n",
    "do_delete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Entrainez le meilleur FullyConnectedNeuralNet que vous pouvez sur les  #\n",
    "#  données CIFAR-10. Vous pourriez trouver la normalization par lots et le     #\n",
    "#  dropout utile. Stockez votre meilleur modèle dans la variable best_model.   #\n",
    "################################################################################\n",
    "\n",
    "def uniform(minv, maxv):\n",
    "    return np.random.rand() * (maxv - minv) + minv\n",
    "\n",
    "if 'trys' not in locals():\n",
    "    trys = []\n",
    "    best_try = -1\n",
    "    best_val_acc = 0\n",
    "for i in range(1):\n",
    "    # reg 0.00779190546432  lr 0.000423844376859\n",
    "    # reg 0.000204588367827 lr 0.000529341067109\n",
    "    weight_scale = 3e-2  # 10 ** uniform(-3, -1)\n",
    "    lr = 10 ** -2.5  # 10 ** uniform(-4, -2)\n",
    "    reg = 1e-10  # 10 ** uniform(-6, -3)\n",
    "    \n",
    "    model = FullyConnectedNeuralNet([100] * 5, weight_scale=weight_scale, reg=reg, use_batchnorm=True)\n",
    "\n",
    "    solver = Solver(model, data,\n",
    "                  num_epochs=30, batch_size=100,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': lr\n",
    "                  },\n",
    "                  print_every=200,\n",
    "                  verbose=True)\n",
    "    solver.train()\n",
    "    \n",
    "    max_val_acc = np.max(solver.val_acc_history)\n",
    "    cur_index = len(trys)\n",
    "    print('Try', cur_index, 'Max accu. val.', max_val_acc, 'p:', weight_scale, reg, lr, ('Record!' if max_val_acc > best_val_acc else ''))\n",
    "    trys.append({\n",
    "            'ws': weight_scale, 'reg': reg, 'lr': lr,\n",
    "            'maxvacc': max_val_acc,\n",
    "            'lossh': solver.loss_history,\n",
    "            'tacch': solver.train_acc_history,\n",
    "            'vacch': solver.val_acc_history,\n",
    "            'model': model\n",
    "        })\n",
    "    \n",
    "    if max_val_acc > best_val_acc:\n",
    "        best_val_acc = max_val_acc\n",
    "        best_try = cur_index\n",
    "        \n",
    "print('Best try:', best_try)\n",
    "print('Total tries', len(trys))\n",
    "################################################################################\n",
    "#                              FIN DE VOTRE CODE                               #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "forget = 0\n",
    "new_tries = trys[forget:]\n",
    "marker_size = 100\n",
    "#x_scatter, x_label = [np.log10(x['ws']) for x in new_tries], 'log weight scale'\n",
    "x_scatter, x_label = [np.log10(x['lr']) for x in new_tries], 'log lr'\n",
    "y_scatter, y_label = [np.log10(x['reg']) for x in new_tries], 'log reg'\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [x['maxvacc'] for x in new_tries] # default size of markers is 20\n",
    "isorted = np.argsort(colors)\n",
    "x_scatter = np.asarray(x_scatter)[isorted]\n",
    "y_scatter = np.asarray(y_scatter)[isorted]\n",
    "colors = np.asarray(colors)[isorted]\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = trys[best_try]\n",
    "best_try = -1\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(trys[best_try]['lossh'], '-')\n",
    "plt.plot(ref['lossh'], '-')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(trys[best_try]['tacch'], '-', label='Train')\n",
    "plt.plot(trys[best_try]['vacch'], '-', label='Valid')\n",
    "plt.plot(ref['tacch'], '-', label='Train ref')\n",
    "plt.plot(ref['vacch'], '-', label='Valid ref')\n",
    "plt.legend(loc='lower center')\n",
    "\n",
    "plt.tight_layout(pad=0, w_pad=0, h_pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model\n",
    "Run your best model on the validation and test sets. You should achieve above 53% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trys[best_try]['model']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "X_val  = data['X_val']\n",
    "y_val  = data['y_val']\n",
    "y_test_pred = np.argmax(best_model.loss(X_test), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(X_val), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == y_val).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
