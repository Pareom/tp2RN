{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "Ici nous explorerons le concept de batch normalization proposé par [3].  Puisque le code vous est fourni au complet, vous n'avez qu'une question à réponse à la fin de ce fichier.\n",
    "\n",
    "[3] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "Internal Covariate Shift\", ICML 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run the following from the ift725 directory and try again:\n",
      "python setup.py build_ext --inplace\n",
      "You may also need to restart your iPython kernel\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ift725.classifiers.fc_net import *\n",
    "from ift725.data_utils import get_CIFAR10_data\n",
    "from ift725.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from ift725.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print ('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization: propagation avant\n",
    "Dans le fichier `ift725/layers.py`, la fonction `forward_batch_normalization` effectue un propagation avant avec batchnorm.  Assurez-vous de bien comprendre le code. Le code que voici permet d'en tester la validité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ 8.00775754 -3.29395872 12.25982769]\n",
      "  stds:  [31.19981052 30.49134475 28.80413928]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [5.60662627e-17 4.23966418e-17 1.49880108e-16]\n",
      "  std:  [0.99999999 0.99999999 0.99999999]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [11. 12. 13.]\n",
      "  stds:  [0.99999999 1.99999999 2.99999998]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print ('Before batch normalization:')\n",
    "print ('  means: ', a.mean(axis=0))\n",
    "print ('  stds: ', a.std(axis=0))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print ('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = forward_batch_normalization(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
    "print ('  mean: ', a_norm.mean(axis=0))\n",
    "print ('  std: ', a_norm.std(axis=0))\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "a_norm, _ = forward_batch_normalization(a, gamma, beta, {'mode': 'train'})\n",
    "print ('After batch normalization (nontrivial gamma, beta)')\n",
    "print ('  means: ', a_norm.mean(axis=0))\n",
    "print ('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555555555555555555555555555555555555555555\n",
      "[1. 1. 1.]\n",
      "After batch normalization (test-time):\n",
      "  means:  [-0.01973341  0.05041113  0.01264676]\n",
      "  stds:  [0.98955024 0.9219021  1.10095016]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "print(\"555555555555555555555555555555555555555555\")\n",
    "print(gamma)\n",
    "beta = np.zeros(D3)\n",
    "for t in range(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  forward_batch_normalization(a, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = forward_batch_normalization(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization: rétro-propagation\n",
    "la rétro-propagation de batch norm est dans la fonction `backward_batch_normalization`.  Ici aussi, assurez-vous de bien comprendre le code.  La cellule suivante effectue une vérification dilligente du gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.22690775  1.31350433  1.07455687  0.47759674 -1.05546122]\n",
      "dx error:  2.2128486530610123e-09\n",
      "dgamma error:  5.308842210964785e-12\n",
      "dbeta error:  3.2755842569322874e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: forward_batch_normalization(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: forward_batch_normalization(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: forward_batch_normalization(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = forward_batch_normalization(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = backward_batch_normalization(dout, cache)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization: alternative backward\n",
    "Voici une version alternative de la batch norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22714669 -0.91513264  0.43867717 -0.3992176  -0.32064567 -0.3604505\n",
      " -1.32925798 -0.19291289 -0.97661347 -0.63585724  1.24348815 -0.14867124\n",
      "  1.00717664 -0.67956821 -0.23012849 -0.56807472  0.37886546  2.00751833\n",
      "  0.37470305  0.51630891  0.94899109 -1.36461992  0.57374361  1.71301791\n",
      "  0.57691632  1.31555933  0.38439969  0.6782703   0.76698012  0.35129751\n",
      "  0.32408491 -1.32288673  1.77238459  1.50396812  0.14188713  0.24977976\n",
      " -0.08455432 -0.38626811  0.17087293  2.44477463  0.39785627  1.20008088\n",
      "  0.6107212   2.66368998  0.42200045  1.20178802 -0.0119712   1.26402417\n",
      "  1.57441231 -0.96686065  0.01774003  0.49618765 -0.37440427  0.08098588\n",
      "  0.90461024 -0.92015601 -0.0947174   0.5281059  -0.34642984  0.34150993\n",
      "  1.144466    1.04014655  1.38867583  0.13253235 -1.09203341  0.19316514\n",
      "  0.81685888  1.20170719 -0.41956849 -0.80003931 -0.55423052  1.39140146\n",
      " -2.21704157  2.20539559 -0.44324751 -0.57614756 -0.74087332 -1.61554123\n",
      " -0.19438874 -0.95246812 -1.90724174  0.1592819   0.28801711 -0.89317035\n",
      "  0.15916607  0.82150878  0.11456775  0.62874688 -0.18560751  0.1154651\n",
      "  0.87709583 -0.50584943  0.10909715  0.2872913   2.48750964  1.49844054\n",
      " -0.68849398 -0.02343951  1.89215758  1.1056865  -0.41430705 -0.89761495\n",
      " -0.11436486 -0.37414632  0.72708685  0.27828143 -1.06793796 -0.06291956\n",
      " -0.22758169 -1.50810096  0.13233335  0.49754989 -1.35858371 -0.35848094\n",
      " -0.71400506  0.3509312   0.3818281   1.03631431  0.62346451 -0.17332212\n",
      " -0.01842933  1.2074817   0.35461018 -0.05451528  0.07112912  0.20901808\n",
      " -0.40857405 -0.67823602  0.29252164  1.49894497  1.71774474 -0.96931184\n",
      " -1.38492462 -1.17516585  0.1464417   1.48308948  1.7223937   1.04688221\n",
      "  1.64695279  0.64301909  0.98780272 -0.49550452  0.22434379  0.05516396\n",
      "  0.82492743 -1.20647061 -0.44112878  0.40205031  1.39291958  0.1539254\n",
      " -0.4086866  -0.90657778  0.31682588  1.48405591  0.81056198  0.18614016\n",
      " -0.38907041  0.71780887 -0.52647885  0.21247199  1.15146689  1.21357288\n",
      " -0.33917581  1.01967029 -1.06910808 -0.49807088 -0.18345837 -0.22208899\n",
      " -1.21681338 -0.41252148 -1.60458155  0.82729312 -0.11124647 -0.9244689\n",
      "  0.86786527  0.40986497  0.57549161  1.75282417 -0.07110929  0.44300309\n",
      " -0.64758547 -0.0601924   0.90106066  0.84598854 -1.17345259 -0.55075072\n",
      " -0.90986712  0.064189   -2.58876121 -0.07000003 -1.36523865 -1.16281966\n",
      "  0.39400927 -0.81427512 -1.65178701 -0.6675225   0.06482978  2.70619098\n",
      " -1.42145014 -1.18625529 -0.0089301  -1.76515113 -0.01622889 -0.37950977\n",
      " -0.40080265 -0.89255755  0.17549432  0.48175448  0.00790076  2.81216286\n",
      " -1.35135745  0.39618023 -0.58128836  2.40156928  0.51393104  0.94980518\n",
      "  1.20107235  0.30674784  0.1215505   0.02499111  0.53775252  1.35903017\n",
      " -0.23983946  0.16621215  0.27041198 -0.65242253 -0.40974963 -1.16138886\n",
      "  1.73175126  1.98149611  3.10878342  2.14969088  1.01409362  0.63082464\n",
      "  0.66806771 -1.47385581 -0.1006517  -0.8869733   0.32238743 -0.08881787\n",
      "  0.29574461 -0.11835025 -1.51805099 -0.60850751 -0.69279816 -0.83347722\n",
      "  0.25710901 -0.80677178  2.7136154  -0.48545154 -1.21331562 -0.787104\n",
      "  0.45374965 -2.43122088 -0.99439536  0.22569308 -0.89628479  1.24160197\n",
      " -0.14203315 -0.72528653 -2.29371395  0.42961705 -0.19833627  2.03029157\n",
      " -1.2161051   0.53027632 -0.80775337 -1.41181748  1.93634183 -1.2701757\n",
      "  0.57941839 -0.40121877 -0.07432572 -0.05371831 -1.47921056 -0.9475365\n",
      "  0.04455295 -1.56720317 -1.84107859 -0.59111183 -0.13277131  0.70233871\n",
      " -0.23965294 -0.48510141  0.00718728  1.88510985 -0.04938364 -0.68760345\n",
      "  0.54025759  0.99982817 -0.32723569  1.83989954 -0.416118    0.20265869\n",
      " -0.72233344 -0.87804991  0.32341495  0.04004062 -0.78718048 -1.1036665\n",
      " -2.22821982 -0.92670236  1.06739623 -0.28531158 -0.20081426 -1.18175652\n",
      "  1.45438235 -0.48038988 -0.47366594  1.0459858   0.42952385  0.3496207\n",
      " -0.98684117 -0.06533338 -0.86935834 -0.6401684  -1.610127   -0.46976742\n",
      " -0.61112082 -1.29989754 -1.02219024 -1.37987208  1.18623992  0.43439359\n",
      " -0.61554362 -0.57738184  0.98745127 -0.50074895  1.12047043 -0.92127826\n",
      " -0.18121657 -1.61708365  1.28455804  0.68171762  0.51315277 -0.92376489\n",
      "  1.1246988   0.11310399 -0.46900039  0.31558616  1.97725822 -0.94328656\n",
      " -1.21273918 -1.11363571  0.23803725 -0.12266369  0.40527165 -0.47405752\n",
      " -1.02785541 -0.37199004  2.66188605 -1.69580233  0.83495441  0.08924255\n",
      " -0.55659915 -0.4422042   0.2215633  -2.07594316  3.0162057  -0.64501163\n",
      "  0.22710544 -0.00736344  0.79176968  0.03023753 -1.18212893 -0.49133902\n",
      "  0.11369527  1.22095706 -1.13556307  2.68658002 -0.07814159 -1.5426455\n",
      " -1.22311666 -0.05632779 -0.5776793  -1.98332138 -0.47572027  0.85131143\n",
      " -0.38260856  0.99766138 -0.12351412 -0.64076403 -1.36261171 -0.74673348\n",
      "  0.92857477 -0.26086832 -0.18803987 -0.2514399  -0.46085522  0.09520362\n",
      "  1.44194119  0.01028236 -0.59125538 -0.0769977  -0.10700913 -2.68897533\n",
      " -1.37264967 -1.75914816 -0.96982767 -0.25959332  2.57232822 -0.00792347\n",
      "  1.06635579 -0.60505165 -0.31590326  2.06738096 -0.47920787  0.06675454\n",
      "  0.419658    0.00953012  0.49251866 -2.91874953  0.17362635 -0.63353587\n",
      " -0.6362049  -0.44016381  0.58634076  0.41873804 -0.65147172  0.75777862\n",
      " -0.24625999  0.36214135 -0.06557713  0.9246553  -1.35182821  0.79483344\n",
      " -0.7723842  -1.42451541  1.30173752 -0.31413493  0.33805277  0.72905926\n",
      " -0.30367053  0.4108804  -0.45737031 -0.02405828  0.64219127  0.29125509\n",
      "  0.98247657 -0.15468106  0.52979768 -0.5766179  -0.55820933 -0.14940524\n",
      " -0.75001167 -0.34935014  0.06535085  0.21117225  0.40876005 -0.30915709\n",
      " -0.7321256  -1.47861197  0.28050073 -1.67512091 -0.67414942  0.81347723\n",
      "  1.17316976  0.42959109 -1.02122171 -1.43136121 -0.57988284  0.29177876\n",
      " -1.38675819  0.24859498  0.31213573 -1.03227903 -0.85001498  0.0731398\n",
      " -1.88556897 -0.57538728 -1.72849696 -0.41592349  0.254242    0.52473648\n",
      " -1.63696737 -1.72805591 -1.90044385 -0.63479176 -1.86983499  0.50002021\n",
      " -2.39512992  0.36264003 -0.00668932 -0.13541172 -1.0890201   0.97204559\n",
      " -1.07901653 -0.03451691  1.13765699 -0.67977908  0.4028216   0.819198\n",
      "  0.27600828  0.02518158  0.54231286 -0.02567935 -0.19679284 -0.20141325\n",
      " -0.04432143  0.50992596]\n",
      "dx difference:  1.4690295863691896e-12\n",
      "dgamma difference:  0.0\n",
      "dbeta difference:  0.0\n",
      "speedup: 7.00x\n"
     ]
    }
   ],
   "source": [
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = forward_batch_normalization(x, gamma, beta, bn_param)\n",
    "\n",
    "t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = backward_batch_normalization(dout, cache)\n",
    "t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = backward_batch_normalization_alternative(dout, cache)\n",
    "t3 = time.time()\n",
    "\n",
    "print('dx difference: ', rel_error(dx1, dx2))\n",
    "print('dgamma difference: ', rel_error(dgamma1, dgamma2))\n",
    "print('dbeta difference: ', rel_error(dbeta1, dbeta2))\n",
    "print('speedup: %.2fx' % ((t2 - t1) / (t3 - t2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Suivant le code de la fonction `forward_batch_normalization` expliquez mathématiquement l'opération effectuée en mode `train` et en mode `test`\n",
    "\n",
    "## Votre réponse ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Suivant le code de la fonction `backward_batch_normalization` expliquez mathématiquement l'opération effectuée.\n",
    "\n",
    "## Votre réponse ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux pleinement connectés et Batch Normalization\n",
    "Maintenant que vous comprenez en quoi consiste la *batch normalization*, allez dans votre classe `FullyConnectedNeuralNet` du fichier `ift725/classifiers/fc_net.py` et modifiez le code afin d'include la batch normalization à vos réseaux de neurones.\n",
    "\n",
    "Plus spécifiquement, lorsque la variable `use_batchnorm` est à `True` dans le constructeur, vous devriez ajouter une couche *batch norma*  **avant** chaque couche ReLU. De plus, la sortie de la dernière couche **ne doit pas** être normalisée. Une fois fait, vérifiez votre implantation avec le code que voici.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "INITIALISATION\n",
      "para name W : W1\n",
      "init premiere couche\n",
      "W hidden layer shape (15, 20)\n",
      "B hidden layer shape (15, 20)\n",
      "para name W : W2\n",
      "init couche cachée\n",
      "W hidden layer shape (20, 30)\n",
      "B hidden layer shape (20, 30)\n",
      "init derniere couche, layer : 3\n",
      "para name W : W3\n",
      "W output layer shape (30, 10)\n",
      "B output layer shape (30, 10)\n",
      "PROPAGATION AVANT\n",
      "layer :  1\n",
      "couche cachée:  1\n",
      "X shape : (2, 15)\n",
      "W shape : (15, 20)\n",
      "B shape : (20,)\n",
      "shape gamma\n",
      "(20,)\n",
      "shape beta\n",
      "(20,)\n",
      "forward_batch_normalization OK\n",
      "forward_batch_normalization + RELU OK\n",
      "layer :  2\n",
      "couche cachée:  2\n",
      "X shape : (2, 20)\n",
      "W shape : (20, 30)\n",
      "B shape : (30,)\n",
      "shape gamma\n",
      "(30,)\n",
      "shape beta\n",
      "(30,)\n",
      "forward_batch_normalization OK\n",
      "forward_batch_normalization + RELU OK\n",
      "derniere couche\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-59287fd4982e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                             use_batchnorm=True)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m   \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m   \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Initial loss: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Sherbrooke 2019-2020\\Session Hiver 2020\\IFT725_réseaux_de_neurones\\tp2RN\\prog\\ift725\\classifiers\\fc_net.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam_name_b\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         scores, self.caches[param_name_cache] = forward_fully_connected_transform_relu(X, self.params[param_name_W],\n\u001b[1;32m--> 424\u001b[1;33m                                                                                        self.params[param_name_b])\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'derniere couche OK'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Sherbrooke 2019-2020\\Session Hiver 2020\\IFT725_réseaux_de_neurones\\tp2RN\\prog\\ift725\\layer_combo.py\u001b[0m in \u001b[0;36mforward_fully_connected_transform_relu\u001b[1;34m(x, w, b)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mObject\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgive\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbackward\u001b[0m \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_fully_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfc_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Sherbrooke 2019-2020\\Session Hiver 2020\\IFT725_réseaux_de_neurones\\tp2RN\\prog\\ift725\\layers.py\u001b[0m in \u001b[0;36mforward_fully_connected\u001b[1;34m(x, w, b)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m#############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0minlineX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minlineX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m#############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m#                             FIN DE VOTRE CODE                             #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 30)"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print ('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNeuralNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64,\n",
    "                            use_batchnorm=True)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print ('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "  if reg == 0: print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécutez ce code pour entraîner un réseau de neurones à 6 couches sur 1000 images de CIFAR10 avec et sans batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "bn_model = FullyConnectedNeuralNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True)\n",
    "model = FullyConnectedNeuralNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "bn_solver = Solver(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "bn_solver.train()\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(solver.loss_history, '-', label='baseline')\n",
    "plt.plot(bn_solver.loss_history, '-', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o', label='baseline')\n",
    "plt.plot(bn_solver.train_acc_history, '-o', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(solver.val_acc_history, '-o', label='baseline')\n",
    "plt.plot(bn_solver.val_acc_history, '-o', label='batchnorm')\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization et initialisation\n",
    "Les dernières cellules de ce notebook a pour objectif d'illustrer l'intéraction qu'il y a entre *batch norm* et l'initialisation d'un réseau de neurones.\n",
    "\n",
    "Dans la première cellule nous entraînerons un réseau à 8 couches avec et sans *batch normalization* avec différentes échelles (*scales*) d'initialisation des poids. Ensuite nous afficherons la justesse en entrainement, en validation ainsi que la perte obtenues pour différents échelles d'initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [50, 50, 50, 50, 50, 50, 50]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "bn_solvers = {}\n",
    "solvers = {}\n",
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "  print('Running weight scale %d / %d' % (i + 1, len(weight_scales)))\n",
    "  bn_model = FullyConnectedNeuralNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True)\n",
    "  model = FullyConnectedNeuralNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "  bn_solver = Solver(bn_model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  bn_solver.train()\n",
    "  bn_solvers[weight_scale] = bn_solver\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  solver.train()\n",
    "  solvers[weight_scale] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of weight scale experiment\n",
    "best_train_accs, bn_best_train_accs = [], []\n",
    "best_val_accs, bn_best_val_accs = [], []\n",
    "final_train_loss, bn_final_train_loss = [], []\n",
    "\n",
    "for ws in weight_scales:\n",
    "  best_train_accs.append(max(solvers[ws].train_acc_history))\n",
    "  bn_best_train_accs.append(max(bn_solvers[ws].train_acc_history))\n",
    "  \n",
    "  best_val_accs.append(max(solvers[ws].val_acc_history))\n",
    "  bn_best_val_accs.append(max(bn_solvers[ws].val_acc_history))\n",
    "  \n",
    "  final_train_loss.append(np.mean(solvers[ws].loss_history[-100:]))\n",
    "  bn_final_train_loss.append(np.mean(bn_solvers[ws].loss_history[-100:]))\n",
    "  \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best val accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best val accuracy')\n",
    "plt.semilogx(weight_scales, best_val_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_val_accs, '-o', label='batchnorm')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Best train accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best training accuracy')\n",
    "plt.semilogx(weight_scales, best_train_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_train_accs, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Final training loss vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Final training loss')\n",
    "plt.semilogx(weight_scales, final_train_loss, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_final_train_loss, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.gcf().set_size_inches(10, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:\n",
    "Quelle(s) conclusion(s) tirez-vous de ces courbes?\n",
    "\n",
    "## Votre réponse:...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
